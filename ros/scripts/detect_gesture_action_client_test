#! /usr/bin/env python
import sys

import rospy
import roslib
import actionlib

import cv2
from sensor_msgs.msg import Image

from mdr_detect_gesture.msg import DetectGestureAction, DetectGestureGoal, DetectGestureFeedback, DetectGestureResult

if __name__ == '__main__':
    rospy.init_node('detect_gesture_client_test')
    if len(sys.argv) != 2:
        print('Usage: detect_gesture_action_client_test <input_image_path>')

    
    # input_image_path = sys.argv[1]
    # input_image_path = 'cv2.VideoCapture(0)'

    client = actionlib.SimpleActionClient('mdr_actions/detect_gesture_server', DetectGestureAction)
    client.wait_for_server()

    goal = DetectGestureGoal()
    
    goal.start = True

    # img = cv2.imread(input_image_path)
    # ros_image = Image()
    # ros_image.height = img.shape[0]
    # ros_image.width = img.shape[1]
    # ros_image.encoding = 'bgr8'
    # ros_image.data = img.flatten().tolist()
    # goal.image = ros_image

    client.send_goal(goal)
    client.wait_for_result()
    print(client.get_result())
